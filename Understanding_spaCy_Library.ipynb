{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Understanding spaCy Library.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXNNSj9CCJoZ"
      },
      "source": [
        "Installation of prerequisite"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m69rVKQRz5AW",
        "outputId": "2d56bbab-3ccc-4736-a7ba-f72ddbae4644"
      },
      "source": [
        "!pip install -U spacy\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting spacy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1b/d8/0361bbaf7a1ff56b44dca04dace54c82d63dad7475b7d25ea1baefafafb2/spacy-3.0.6-cp37-cp37m-manylinux2014_x86_64.whl (12.8MB)\n",
            "\u001b[K     |████████████████████████████████| 12.8MB 186kB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied, skipping upgrade: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.11.3)\n",
            "Collecting catalogue<2.1.0,>=2.0.3\n",
            "  Downloading https://files.pythonhosted.org/packages/9c/10/dbc1203a4b1367c7b02fddf08cb2981d9aa3e688d398f587cea0ab9e3bec/catalogue-2.0.4-py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.5)\n",
            "Requirement already satisfied, skipping upgrade: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.41.1)\n",
            "Collecting srsly<3.0.0,>=2.4.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c3/84/dfdfc9f6f04f6b88207d96d9520b911e5fec0c67ff47a0dea31ab5429a1e/srsly-2.4.1-cp37-cp37m-manylinux2014_x86_64.whl (456kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 17.5MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: typing-extensions<4.0.0.0,>=3.7.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from spacy) (3.7.4.3)\n",
            "Requirement already satisfied, skipping upgrade: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (20.9)\n",
            "Collecting pydantic<1.8.0,>=1.7.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ca/fa/d43f31874e1f2a9633e4c025be310f2ce7a8350017579e9e837a62630a7e/pydantic-1.7.4-cp37-cp37m-manylinux2014_x86_64.whl (9.1MB)\n",
            "\u001b[K     |████████████████████████████████| 9.1MB 42.6MB/s \n",
            "\u001b[?25hCollecting thinc<8.1.0,>=8.0.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1c/83/1f567d77173dcdf8e57fccd2a9e086d7702f4b42299070506f72d7353d3a/thinc-8.0.6-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (618kB)\n",
            "\u001b[K     |████████████████████████████████| 624kB 40.4MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.0.0)\n",
            "Collecting pathy>=0.3.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/13/87/5991d87be8ed60beb172b4062dbafef18b32fa559635a8e2b633c2974f85/pathy-0.5.2-py3-none-any.whl (42kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 6.2MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.8.2)\n",
            "Collecting spacy-legacy<3.1.0,>=3.0.4\n",
            "  Downloading https://files.pythonhosted.org/packages/fa/a5/a727792d000b2a7bfcccbad03b292cd4c2d567d271fc3cab91250c2461e8/spacy_legacy-3.0.6-py2.py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.5)\n",
            "Collecting typer<0.4.0,>=0.3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/90/34/d138832f6945432c638f32137e6c79a3b682f06a63c488dcfaca6b166c64/typer-0.3.2-py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy) (2.0.1)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.3->spacy) (3.4.1)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.5.30)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy) (2.4.7)\n",
            "Collecting smart-open<4.0.0,>=2.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/11/9a/ba2d5f67f25e8d5bbf2fcec7a99b1e38428e83cb715f64dd179ca43a11bb/smart_open-3.0.0.tar.gz (113kB)\n",
            "\u001b[K     |████████████████████████████████| 122kB 55.1MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy) (7.1.2)\n",
            "Building wheels for collected packages: smart-open\n",
            "  Building wheel for smart-open (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for smart-open: filename=smart_open-3.0.0-cp37-none-any.whl size=107107 sha256=8a72fbb918983d2b691a7de152457d62076121605dc73ab608fac935bf9a170d\n",
            "  Stored in directory: /root/.cache/pip/wheels/18/88/7c/f06dabd5e9cabe02d2269167bcacbbf9b47d0c0ff7d6ebcb78\n",
            "Successfully built smart-open\n",
            "Installing collected packages: catalogue, srsly, pydantic, thinc, typer, smart-open, pathy, spacy-legacy, spacy\n",
            "  Found existing installation: catalogue 1.0.0\n",
            "    Uninstalling catalogue-1.0.0:\n",
            "      Successfully uninstalled catalogue-1.0.0\n",
            "  Found existing installation: srsly 1.0.5\n",
            "    Uninstalling srsly-1.0.5:\n",
            "      Successfully uninstalled srsly-1.0.5\n",
            "  Found existing installation: thinc 7.4.0\n",
            "    Uninstalling thinc-7.4.0:\n",
            "      Successfully uninstalled thinc-7.4.0\n",
            "  Found existing installation: smart-open 5.1.0\n",
            "    Uninstalling smart-open-5.1.0:\n",
            "      Successfully uninstalled smart-open-5.1.0\n",
            "  Found existing installation: spacy 2.2.4\n",
            "    Uninstalling spacy-2.2.4:\n",
            "      Successfully uninstalled spacy-2.2.4\n",
            "Successfully installed catalogue-2.0.4 pathy-0.5.2 pydantic-1.7.4 smart-open-3.0.0 spacy-3.0.6 spacy-legacy-3.0.6 srsly-2.4.1 thinc-8.0.6 typer-0.3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mmsVJM_IB1ZN",
        "outputId": "3eb11e4d-a6a6-4b98-b42a-2910d287959a"
      },
      "source": [
        "!pip install -U spacy-lookups-data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting spacy-lookups-data\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6f/14/56f7bdf61db1550ed398037f5a5173670b4bae5fec2570b537aaafe9ec65/spacy_lookups_data-1.0.2-py2.py3-none-any.whl (97.3MB)\n",
            "\u001b[K     |████████████████████████████████| 97.3MB 37kB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy-lookups-data) (57.0.0)\n",
            "Installing collected packages: spacy-lookups-data\n",
            "Successfully installed spacy-lookups-data-1.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2bjdJhzCCN6",
        "outputId": "60c9a3c3-8bc0-493a-ee90-84a53334ddc7"
      },
      "source": [
        "!python -m spacy download en_core_web_sm                   #Download Pretrained Model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-06-23 14:53:40.658294: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "Collecting en-core-web-sm==3.0.0\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.0.0/en_core_web_sm-3.0.0-py3-none-any.whl (13.7MB)\n",
            "\u001b[K     |████████████████████████████████| 13.7MB 218kB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.1.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-sm==3.0.0) (3.0.6)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.19.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.0.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (20.9)\n",
            "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.3.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (4.41.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.5)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.4.1)\n",
            "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.7.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.23.0)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.4.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.0.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.11.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (57.0.0)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (8.0.6)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.7.4.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.8.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.6)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.0.4)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.5.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.4.7)\n",
            "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (7.1.2)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.24.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.0.1)\n",
            "Requirement already satisfied: zipp>=0.5; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.3->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.4.1)\n",
            "Requirement already satisfied: smart-open<4.0.0,>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.0)\n",
            "Installing collected packages: en-core-web-sm\n",
            "  Found existing installation: en-core-web-sm 2.2.5\n",
            "    Uninstalling en-core-web-sm-2.2.5:\n",
            "      Successfully uninstalled en-core-web-sm-2.2.5\n",
            "Successfully installed en-core-web-sm-3.0.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDUKCdw6Cgb6"
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xk4zFYJQCqLx"
      },
      "source": [
        "Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YaLxDi8CClvS",
        "outputId": "0271fc34-ee6f-4d96-c991-5082bf395e82"
      },
      "source": [
        "doc = nlp(\"Apple isn't looking at buyig U.K. startup for $1 billion\")\n",
        "for token in doc:\n",
        "    print(token.text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Apple\n",
            "is\n",
            "n't\n",
            "looking\n",
            "at\n",
            "buyig\n",
            "U.K.\n",
            "startup\n",
            "for\n",
            "$\n",
            "1\n",
            "billion\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MdonoiJC2bL"
      },
      "source": [
        "Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "deV1EqhrC5aK",
        "outputId": "c8bf5e78-4797-463d-a107-16a1860f7b32"
      },
      "source": [
        "for token in doc:\n",
        "    print(token.text, token.lemma_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Apple Apple\n",
            "is be\n",
            "n't n't\n",
            "looking look\n",
            "at at\n",
            "buyig buyig\n",
            "U.K. U.K.\n",
            "startup startup\n",
            "for for\n",
            "$ $\n",
            "1 1\n",
            "billion billion\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIfbdI-HDGzT"
      },
      "source": [
        "Part-of-speech tagging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bKZ41MQODKRB",
        "outputId": "79d6becd-86bf-4c24-9026-94422b2fcff5"
      },
      "source": [
        "for token in doc:\n",
        "    print(f'{token.text:{15}} {token.lemma_:{15}} {token.pos_:{10}}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Apple           Apple           PROPN     \n",
            "is              be              AUX       \n",
            "n't             n't             PART      \n",
            "looking         look            VERB      \n",
            "at              at              ADP       \n",
            "buyig           buyig           NOUN      \n",
            "U.K.            U.K.            PROPN     \n",
            "startup         startup         NOUN      \n",
            "for             for             ADP       \n",
            "$               $               SYM       \n",
            "1               1               NUM       \n",
            "billion         billion         NUM       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7vWQ1DjDlSp"
      },
      "source": [
        "Dependency Parsing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EbtvNn8zDnmq",
        "outputId": "f962b450-28e7-4e6e-f704-e9637400e235"
      },
      "source": [
        "for chunk in doc.noun_chunks:\n",
        "    print(f'{chunk.text:{30}} {chunk.root.text:{15}} {chunk.root.dep_}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Apple                          Apple           nsubj\n",
            "buyig                          buyig           pobj\n",
            "U.K.                           U.K.            nsubj\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6OfJZ_JTEPTS"
      },
      "source": [
        "Named Entity Recognition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XWYLT5WuEUIq",
        "outputId": "7c10945a-1e8c-43ce-807e-b097888d897e"
      },
      "source": [
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.label_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Apple ORG\n",
            "U.K. GPE\n",
            "$1 billion MONEY\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwXXZTc0EkVo"
      },
      "source": [
        "Sentence Segmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "benYOMdKEjRR",
        "outputId": "197adf42-8a40-492d-c0da-f7bd850b7921"
      },
      "source": [
        "for sent in doc.sents:\n",
        "    print(sent)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Apple isn't looking at buyig U.K. startup for $1 billion\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hni4aPlaErAM",
        "outputId": "3643ab96-244f-411e-f01d-11983b7a304d"
      },
      "source": [
        "doc1 = nlp(\"Welcome to KGP Talkie. Thanks for watching. Please like and subscribe\")\n",
        "for sent in doc1.sents:\n",
        "    print(sent)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Welcome to KGP Talkie.\n",
            "Thanks for watching.\n",
            "Please like and subscribe\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KYVBaU5bEx3C",
        "outputId": "8251cd81-0f89-46ad-b08c-cd19773852ef"
      },
      "source": [
        "doc2 = nlp(\"Welcome to.*.KGP Talkie.*.Thanks for watching\")                     #delimiter cause failure in sentence Segmentation\n",
        "for sent in doc2.sents:\n",
        "    print(sent)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Welcome to.*.KGP Talkie.*.Thanks for watching\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kWUwBvKRYTE2",
        "outputId": "217340fd-db40-4956-d6c0-5d80ea12d396"
      },
      "source": [
        "nlp.pipeline"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x7f5742809d10>),\n",
              " ('tagger', <spacy.pipeline.tagger.Tagger at 0x7f574281b3b0>),\n",
              " ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x7f5742a5f8a0>),\n",
              " ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x7f5742a5f9f0>),\n",
              " ('attribute_ruler',\n",
              "  <spacy.pipeline.attributeruler.AttributeRuler at 0x7f574274d0f0>),\n",
              " ('lemmatizer',\n",
              "  <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x7f5742753050>)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S9So_ksMS-Fl",
        "outputId": "249afd7e-8383-48b0-e354-76f679e8ba30"
      },
      "source": [
        "from spacy.language import Language\n",
        "@Language.component(\"set_rule\")\n",
        "def set_rule(doc):\n",
        "    for token in doc[:-1]:\n",
        "        if token.text == '...':\n",
        "            doc[token.i + 1].is_sent_start = True\n",
        "    return doc\n",
        "\n",
        "ruler = nlp.add_pipe(\"set_rule\",before='parser')\n",
        "\n",
        "text = 'Welcome to KGP Talkie...Thanks...Like and Subscribe!'\n",
        "doc = nlp(text)\n",
        "for sent in doc.sents:\n",
        "    print(sent)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Welcome to KGP Talkie...\n",
            "Thanks...\n",
            "Like and Subscribe!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3ikSDKXawwq",
        "outputId": "5cc82317-a879-40ad-ade7-b8d11ce4f0be"
      },
      "source": [
        "nlp.pipeline"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x7f5742809d10>),\n",
              " ('tagger', <spacy.pipeline.tagger.Tagger at 0x7f574281b3b0>),\n",
              " ('set_rule', <function __main__.set_rule>),\n",
              " ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x7f5742a5f8a0>),\n",
              " ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x7f5742a5f9f0>),\n",
              " ('attribute_ruler',\n",
              "  <spacy.pipeline.attributeruler.AttributeRuler at 0x7f574274d0f0>),\n",
              " ('lemmatizer',\n",
              "  <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x7f5742753050>)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8C4llBubbEL2",
        "outputId": "6092f49a-c363-484c-ac65-e043cbd47477"
      },
      "source": [
        "for token in doc:\n",
        "    print(token.text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Welcome\n",
            "to\n",
            "KGP\n",
            "Talkie\n",
            "...\n",
            "Thanks\n",
            "...\n",
            "Like\n",
            "and\n",
            "Subscribe\n",
            "!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQnUkGbxbvDB"
      },
      "source": [
        "Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dCc8FV9RerW3"
      },
      "source": [
        "from spacy import displacy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irriIRfdetAx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5973a69a-542e-40fe-f326-57568d0bc1a4"
      },
      "source": [
        "displacy.serve(doc, style='dep')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Using the 'dep' visualizer\n",
            "Serving on http://0.0.0.0:5000 ...\n",
            "\n",
            "Shutting down server on port 5000.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bp6dSPkYoC8Q"
      },
      "source": [
        "2.Rule-Based Phrase Text Extraction and Matching"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KDbMDr5wt1BI",
        "outputId": "5f063b3e-2ee9-4581-e897-f8e92dc32242"
      },
      "source": [
        "#Our objective here is to extract the phrase\"online business\" from the text.So, our objective is that \n",
        "#whenever “online” is followed by the word “business”, then the matcher should be able to find this pattern in the text.\n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "from spacy.matcher import Matcher       #import spaCy matcher\n",
        "matcher = Matcher(nlp.vocab)            ## Initialize the matcher with the spaCy vocabulary\n",
        "doc = nlp(\"Amazon is standalone online business company and it is escalating its service all over globe\")\n",
        "pattern=[{'TEXT':'online'},{'TEXT':'business'}]     #Define rule # here 'TEXT'is token attributes.\n",
        "matcher.add('rule_1',[pattern])                     #Add rule\n",
        "matches = matcher(doc)\n",
        "matches"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(7604275899133490726, 3, 5)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IbBgVOHxu7i5",
        "outputId": "446cf76c-450f-4017-fd32-cc079a869d62"
      },
      "source": [
        "# The output has three elements.The first element is the match id.\n",
        "#The second and third elements are the positions of the matched tokens.\n",
        "# Extract matched text\n",
        "for match_id, start, end in matches:\n",
        "    # Get the matched span\n",
        "    matched_span = doc[start:end]\n",
        "    print(matched_span.text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "online business\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWU5wrquvNm6"
      },
      "source": [
        "#TEXT is token attribute refers to exact text in token.\n",
        "#There are, in fact, many other useful token attributes in spaCy which can be used to define a variety of rules and patterns.\n",
        "1.ORTH\tunicode\tThe exact verbatim text of a token.\n",
        "2.TEXT\tunicode\tThe exact verbatim text of a token.\n",
        "3.LOWER\tunicode\tThe lowercase form of the token text.\n",
        "4.LENGTH\tint\tThe length of the token text.\n",
        "5.IS_ALPHA, IS_ASCII, IS_DIGIT\tbool\tToken text consists of alphabetic characters, ASCII characters, digits.\n",
        "6.IS_LOWER, IS_UPPER, IS_TITLE\tbool\tToken text is in lowercase, uppercase, titlecase.\n",
        "7.IS_PUNCT, IS_SPACE, IS_STOP\tbool\tToken is punctuation, whitespace, stop word.\n",
        "8.LIKE_NUM, LIKE_URL, LIKE_EMAIL\tbool\tToken text resembles a number, URL, email.\n",
        "9.POS, TAG, DEP, LEMMA, SHAPE\tunicode\tThe token’s simple and extended part-of-speech tag, dependency label, lemma, shape.\n",
        "10.ENT_TYPE\tunicode\tThe token’s entity label."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1qg-7qqSvoLa",
        "outputId": "fa0c31cb-0bef-4bb9-867e-9a27b7fd52c6"
      },
      "source": [
        "#spaCy Matcher :example 2\n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "from spacy.matcher import Matcher       \n",
        "matcher = Matcher(nlp.vocab)            \n",
        "doc = nlp(\"Amazon is ranked number 1 online business company! and it is escalating its service all over globe\")\n",
        "pattern=[{'IS_DIGIT':True},{'LOWER':'online'},{'LOWER':'business'},{'LOWER':'company'},{'IS_PUNCT':True}]     \n",
        "matcher.add('rule_1',[pattern])                     \n",
        "matches = matcher(doc)\n",
        "matches\n",
        "for match_id, start, end in matches:\n",
        "    matched_span = doc[start:end]\n",
        "    print(matched_span.text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 online business company!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-DWwHNUv3ZQ",
        "outputId": "9a4ae84b-9c65-4fee-f443-567ae46177dd"
      },
      "source": [
        "#spaCy Matcher :example 2\n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "from spacy.matcher import Matcher       \n",
        "matcher = Matcher(nlp.vocab)            \n",
        "doc = nlp(\"I loved dogs but I love cats also\")\n",
        "pattern=[{'LEMMA':'love',\"POS\":\"VERB\"},{\"POS\":\"NOUN\"}]     \n",
        "matcher.add('rule_1',[pattern])                     \n",
        "matches = matcher(doc)\n",
        "matches\n",
        "for match_id, start, end in matches:\n",
        "    matched_span = doc[start:end]\n",
        "    print(matched_span.text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loved dogs\n",
            "love cats\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q4I9wz4vwGBY",
        "outputId": "93e73724-6dda-497a-def0-dcd16a91bc8b"
      },
      "source": [
        "#spaCy Matcher :example 2\n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "from spacy.matcher import Matcher       \n",
        "matcher = Matcher(nlp.vocab)            \n",
        "doc = nlp(\"I bought a smartphone.Now I am buying apps\")\n",
        "pattern=[{'LEMMA':'buy'},{\"POS\":\"DET\",\"OP\":\"?\"},{\"POS\":\"NOUN\"}]     # \"OP\">>Operator playing essential rules\n",
        "matcher.add('rule_1',[pattern])                     \n",
        "matches = matcher(doc)\n",
        "matches\n",
        "for match_id, start, end in matches:\n",
        "    matched_span = doc[start:end]\n",
        "    print(matched_span.text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bought a smartphone\n",
            "buying apps\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ag6nptQ0wS-Q"
      },
      "source": [
        "{\"OP\":\"!\"} >>>> Negation:match 0 times\n",
        "{\"OP\":\"?\"} >>>> Optional:match 0 or 1 times\n",
        "{\"OP\":\"+\"} >>>> Match 1 or more times\n",
        "{\"OP\":\"*\"} >>>> Match 0 or more times"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FKTT2PEwhSY"
      },
      "source": [
        "                                                                     # Shared Vocab and string store #\n",
        "#Strings are only stored at once in Stringstore via nlp.vocab.strings\n",
        "#Vocab:stores data shared across multiple documents\n",
        "#To save memory spaCy encodes all strings to hashvalues"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aY6mHURZw4l8",
        "outputId": "97b308fb-6183-489a-ea7f-f5981ad2dc98"
      },
      "source": [
        "doc = nlp(\"I hate sugar\")\n",
        "print(\"hash value:\",nlp.vocab.strings[\"sugar\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hash value: 5345933910893937740\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dBi3uPKUziG0",
        "outputId": "9fbb58b4-c01d-4864-9d36-dc5e2c4449a6"
      },
      "source": [
        "print(\"string value:\",nlp.vocab.strings[5345933910893937740])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "string value: sugar\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A3EaNBFyzoiZ",
        "outputId": "dff1b0da-10d7-4dca-d4dc-a36bd5b32650"
      },
      "source": [
        "doc = nlp(\"He loves sugar\")\n",
        "print(\"hash value:\",nlp.vocab.strings[\"sugar\"])\n",
        "# hash value of a particular word never changes once assigned at stringstore."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hash value: 5345933910893937740\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jmQqQQ17zqPo",
        "outputId": "c80a44ca-b5bc-47f6-bb03-9995f05fa39a"
      },
      "source": [
        "                                            #Lexemes#\n",
        "#Act like a token\n",
        "#contains context independent information about a word ie.text,orth,is_alpha\n",
        "#but not supports with NET,POS tags,dependency tags.\n",
        "doc = nlp(\"They are good\")\n",
        "lexeme = nlp.vocab[\"good\"]\n",
        "print(lexeme.text,lexeme.orth,lexeme.is_alpha)           # orth >>>hash"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "good 5711639017775284443 True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sMPyei_cz4d5",
        "outputId": "d7d3d417-7816-4139-cc84-ae09eb3a49bf"
      },
      "source": [
        "                                         #Doc and Span object#\n",
        "#Doc object:\n",
        "#create an nlp object\n",
        "from spacy.lang.en import English\n",
        "nlp=English()\n",
        "#Import the Doc class\n",
        "from spacy.tokens import Doc\n",
        "#Define words and spaces to create the Doc \n",
        "words = [\"I\",\"Love\",\"You\",\"!\"]\n",
        "spaces = [True,True,False,True]\n",
        "#create the doc manually\n",
        "doc=Doc(nlp.vocab,words=words,spaces=spaces)\n",
        "doc"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "I Love You! "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30C2zNeV0Tig",
        "outputId": "be72db35-dcf3-445e-91c8-f07aed23d817"
      },
      "source": [
        "#Span object:\n",
        "from spacy.tokens import Doc, Span\n",
        "#Define words and spaces to create the Doc \n",
        "words = [\"I\",\"Love\",\"You\",\"!\"]\n",
        "spaces = [True,True,False,True]\n",
        "#create the doc manually\n",
        "doc=Doc(nlp.vocab,words=words,spaces=spaces)\n",
        "#create a span manually\n",
        "span = Span(doc,0,4)\n",
        "#create a span with a label\n",
        "span_with_label = Span(doc,0,4,label=\"GREETINGS\")\n",
        "#Add span to the doc.ents\n",
        "doc.ents = [span_with_label]\n",
        "span_with_label"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "I Love You!"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hyBTQRsp0uCZ",
        "outputId": "a60b1285-b26d-4e7b-8ab9-5ca45df3cc56"
      },
      "source": [
        "                                                                           ###WordVectors and Semantic Similarity###\n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')  #Load the larger model wih vectors\n",
        "# compare two documents\n",
        "doc1 = nlp(\"I love you\")\n",
        "doc2 = nlp(\"I like her\")\n",
        "print(doc1.similarity(doc2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6988588189436479\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  import sys\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "205aSVzN1GSR",
        "outputId": "d8e8439c-b08f-4122-9521-5f8407cfa25f"
      },
      "source": [
        "#compare two tokens:\n",
        "doc = nlp(\"I am your best buddy and good friend\")\n",
        "token1 = doc[4]\n",
        "token2 = doc[7]\n",
        "print(token1.similarity(token2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.5727074\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"\"\"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NtFBGtjU1lR7",
        "outputId": "1001b3ca-fd64-4fd8-af2e-e2964f8a357f"
      },
      "source": [
        "#compare the document with token:\n",
        "doc=nlp(\"Eating pizza is delicious\")\n",
        "token = nlp(\"mouthwatering\")[0]                      #[0] indicates only one token here\n",
        "print(doc.similarity(token))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.5562266199094733\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5xDLc95y2qjh",
        "outputId": "934282ef-8f5c-4bf3-f793-6e215d6e8438"
      },
      "source": [
        "#compare a span with document:\n",
        "span =nlp(\"Killing you is my pleasure\")[:2]\n",
        "doc = nlp(\"I murder you\")\n",
        "print(span.similarity(doc))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.40768989886351886\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Span.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QJjFwNst276K",
        "outputId": "a907a042-45ed-4869-fc22-8db2f9aa1812"
      },
      "source": [
        "# get Word vectors for token\n",
        "doc = nlp(\"I love to smoke\")\n",
        "print(doc[3].vector)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 4.6002531e-01 -1.3896936e-01 -3.2897794e-01 -4.4410977e-01\n",
            "  1.7561533e-01  5.5847907e-01  5.8112001e-01  4.1435227e-02\n",
            "  2.5891773e-03  1.9897950e-01 -8.1448209e-01 -2.9495826e-01\n",
            " -9.8909062e-01 -7.3401880e-01 -4.5064211e-01  3.1827244e-01\n",
            "  4.9676132e-01 -1.5627649e-01 -2.4227409e-01 -1.4133400e+00\n",
            "  4.3092537e-01  3.4501392e-01 -3.4436584e-03 -3.6153108e-02\n",
            " -7.7259845e-01  3.7085968e-01 -2.3745447e-03  2.7085125e-01\n",
            " -7.6041508e-01  8.1046975e-01 -2.2029664e-01 -8.0547279e-01\n",
            " -6.4894903e-01 -1.3749130e+00 -2.4269047e-01  1.0145135e+00\n",
            "  2.1978538e+00  5.3850305e-01  1.5838724e-01  4.6283323e-01\n",
            "  1.0062516e-01  1.0922542e-01 -8.5831866e-02  2.0414586e+00\n",
            " -3.5790998e-01 -8.7596208e-02  1.3996069e+00  1.3067659e+00\n",
            "  1.0165861e-01 -6.3525236e-01  6.3299441e-01  1.2635080e+00\n",
            "  5.2385062e-01  8.3224791e-01 -4.4243723e-02 -9.3444204e-01\n",
            " -6.4629436e-02 -3.5427369e-02 -5.4238546e-01 -1.9027771e-01\n",
            " -5.4606102e-02  7.1640337e-01 -1.1045464e+00  6.6203558e-01\n",
            " -4.9729532e-01  2.5904050e-01  6.9078296e-02 -1.3931873e+00\n",
            " -3.9945778e-01  9.5713180e-01  7.5219351e-01  7.4095078e-02\n",
            "  6.1144114e-01 -1.3226402e+00 -5.7454962e-01  4.3487191e-01\n",
            " -8.4903342e-01  9.5017099e-01 -7.3810637e-01  1.3887596e-01\n",
            " -3.5632890e-01 -3.5757628e-01  8.7774050e-01 -3.4249461e-01\n",
            " -4.7392419e-01  9.6488583e-01 -9.2702806e-01 -2.2917400e-01\n",
            "  2.2546802e-01 -1.7679624e-02 -1.3020486e+00 -8.2330406e-04\n",
            " -1.3941647e+00  1.1217778e+00  4.7529209e-01 -1.2500111e+00]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1pOd6iA-3OfL",
        "outputId": "9ced94d8-610c-4831-f83f-7b4834dc95e4"
      },
      "source": [
        "                                                                  ###Combining models and rules###\n",
        "#Rule based Matching \n",
        "from spacy.matcher import Matcher\n",
        "matcher = Matcher(nlp.vocab)\n",
        "doc = nlp(\"I love cats and I'm very very happy\")\n",
        "# Patterns are list of dictionaries describing tokens\n",
        "pattern1 = [{\"LEMMA\":\"love\",\"POS\":\"VERB\"},{\"LOWER\":\"cats\"}]\n",
        "matcher.add(\"LOVE_CATS\", [pattern1])\n",
        "# Operators can specify how often a token should be matched\n",
        "pattern2 = [{\"TEXT\":\"very\",\"OP\":\"+\"},{\"TEXT\":\"happy\"}]              #{\"OP\":\"+\"} >>>> Match 1 or more times\n",
        "matcher.add(\"VERY_HAPPY\", [pattern2])\n",
        "#calling matcher on doc\n",
        "matches=matcher(doc)\n",
        "matches\n",
        "for match_id, start, end in matches:\n",
        "    matched_span = doc[start:end]\n",
        "    print(matched_span.text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "love cats\n",
            "very happy\n",
            "very very happy\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iwEt9DqL4RTC",
        "outputId": "d587352a-289b-469d-c05d-5af87481f534"
      },
      "source": [
        "#Adding statistical predictions\n",
        "matcher = Matcher(nlp.vocab)\n",
        "pattern = [{\"LOWER\":\"golden\"},{\"LOWER\":\"retreiver\"}]\n",
        "matcher.add('DOC',[pattern])\n",
        "doc = nlp(\"I have a golden retreiver\")\n",
        "for match_id,start,end in matcher(doc):\n",
        "    span=doc[start:end]\n",
        "    print(\"Matched span:\",span.text)\n",
        "    print(\"Root token:\",span.root.text)                   # For root and root head\n",
        "    print(\"Root head token:\",span.root.head.text)\n",
        "    print(\"Previous token:\",doc[start-1].text,doc[start-1].pos_)       # for previous token and its pos tag"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Matched span: golden retreiver\n",
            "Root token: retreiver\n",
            "Root head token: have\n",
            "Previous token: a DET\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zRGoUmtD4xgK",
        "outputId": "525a0f1a-7f0e-4e02-90fd-35e6bf33f10a"
      },
      "source": [
        "#Adding phrase Matching\n",
        "from spacy.matcher import PhraseMatcher\n",
        "matcher = PhraseMatcher(nlp.vocab)\n",
        "pattern=nlp(\"Golden Retreiver\")                      # define pattern with phrase\n",
        "matcher.add(\"DOC\",[pattern])\n",
        "doc = nlp(\"I have a Golden Retreiver\")\n",
        "for match_id,start,end in matcher(doc):\n",
        "    span=doc[start:end]\n",
        "    print(\"Matched span:\",span.text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Matched span: Golden Retreiver\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJxBlZTU9AtD"
      },
      "source": [
        "With Test Dataset - Understanding rule based Matching and Regular Expression "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5SAJMn58_an"
      },
      "source": [
        "data = open('/content/drive/MyDrive/11-0.txt').read()    # https://www.gutenberg.org/files/11/11-0.txt\n",
        "doc = nlp(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13RDMr089d3r",
        "outputId": "48d8362d-fdf9-4e93-c343-680e8e5d04eb"
      },
      "source": [
        "matcher = Matcher(nlp.vocab)\n",
        "pattern = [{\"TEXT\": \"Alice\"}, {\"POS\": \"VERB\"}]\n",
        "matcher.add(\"alice\",[pattern])                 #the first variable is a unique id for the pattern (alice)\n",
        "matches = matcher(doc)\n",
        "print(\"Matches:\", [doc[start:end].text for match_id, start, end in matches])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Matches: ['Alice think', 'Alice started', 'Alice had', 'Alice had', 'Alice began', 'Alice opened', 'Alice ventured', 'Alice felt', 'Alice took', 'Alice thought', 'Alice had', 'Alice went', 'Alice went', 'Alice thought', 'Alice kept', 'Alice had', 'Alice thought', 'Alice called', 'Alice replied', 'Alice began', 'Alice guessed', 'Alice said', 'Alice went', 'Alice knew', 'Alice heard', 'Alice thought', 'Alice heard', 'Alice noticed', 'Alice dodged', 'Alice looked', 'Alice looked', 'Alice replied', 'Alice replied', 'Alice felt', 'Alice turned', 'Alice thought', 'Alice replied', 'Alice folded', 'Alice said', 'Alice waited', 'Alice crouched', 'Alice noticed', 'Alice laughed', 'Alice went', 'Alice thought', 'Alice said', 'Alice said', 'Alice glanced', 'Alice caught', 'Alice looked', 'Alice did', 'Alice added', 'Alice felt', 'Alice remarked', 'Alice waited', 'Alice coming', 'Alice looked', 'Alice said', 'Alice thought', 'Alice considered', 'Alice replied', 'Alice felt', 'Alice replied', 'Alice sighed', 'Alice asked', 'Alice ventured', 'Alice tried', 'Alice replied', 'Alice said', 'Alice was', 'Alice said', 'Alice thought', 'Alice looked', 'Alice recognised', 'Alice joined', 'Alice gave', 'Alice thought', 'Alice found', 'Alice began', 'Alice waited', 'Alice put', 'Alice began', 'Alice thought', 'Alice appeared', 'Alice ventured', 'Alice whispered', 'Alice thought', 'Alice remarked', 'Alice said', 'Alice said', 'Alice looked', 'Alice was', 'Alice heard', 'Alice thought', 'Alice asked', 'Alice ventured', 'Alice went', 'Alice began', 'Alice replied', 'Alice looked', 'Alice asked', 'Alice began', 'Alice said', 'Alice said', 'Alice was', 'Alice panted', 'Alice whispered', 'Alice began', 'Alice felt', 'Alice guessed', 'Alice watched', 'Alice looked', 'Alice got']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1djKwP9Z-mPn",
        "outputId": "131b1671-07c6-487d-b452-285aba11e8e5"
      },
      "source": [
        "matcher = Matcher(nlp.vocab)\n",
        "pattern = [{\"POS\": \"ADJ\"}, {\"POS\": \"NOUN\"}] #Adjective follwed by noun is our requirement from Data\n",
        "matcher.add(\"id1\",[pattern])\n",
        "matches = matcher(doc)\n",
        "print(\"Matches:\", set([doc[start:end].text for match_id, start, end in matches][:50]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Matches: {'little thing', 'good advice', 'several things', 'right size', 'hot day', 'cool fountains', 'little bit', 'legged table', 'little girl', 'curious feeling', 'small passage', 'long passage', 'simple rules', 'little door', 'lovely garden', 'own mind', 'unpleasant things', 'curious child', 'fancy _', 'golden key', 'dreamy sort', 'grand words', 'good opportunity', 'right distance', 'few things', 'right word', 'dark hall', 'little glass', 'hot poker', 'many miles', 'small cake', 'few minutes', 'low hall', 'first thought', 'own ears', 'low curtain', 'wild beasts', 'little use', 'pink eyes', 'other parts', 'little histories', 'dry leaves', 'loveliest garden', 'little bottle', 'great delight', 'large rabbit'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tTmY2BmG_bKE",
        "outputId": "17c2bbd3-61a3-419d-de21-4c3c7e441bac"
      },
      "source": [
        "matcher = Matcher(nlp.vocab)\n",
        "pattern = [{\"LEMMA\": \"begin\"},{\"POS\": \"ADP\"}]     # Lemma of begin follwed by Adposition\n",
        "matcher.add(\"id1\",[pattern])\n",
        "matches = matcher(doc)\n",
        "print(\"Matches:\", set([doc[start:end].text for match_id, start, end in matches]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Matches: {'began by', 'beginning to', 'begin at', 'began in', 'Begin at', 'beginning with', 'begin with', 'beginning from', 'begins with'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R5RKvIAmAMvH",
        "outputId": "ce63394f-e2c2-4273-8ba6-279dd50ab6ad"
      },
      "source": [
        "matcher = Matcher(nlp.vocab)\n",
        "pattern = [{\"TEXT\": \"Alice\"}, {\"IS_PUNCT\": True,\"OP\":\"*\"}]\n",
        "matcher.add(\"id1\",[pattern])\n",
        "matches = matcher(doc)\n",
        "print(\"Matches:\", set([doc[start:end].text for match_id, start, end in matches]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Matches: {'Alice;', 'Alice. “', 'Alice,', 'Alice: “—', 'Alice, (', 'Alice.', 'Alice,)', 'Alice', 'Alice!”', 'Alice,) “', 'Alice; “', 'Alice (', 'Alice:', 'Alice: “', 'Alice!', 'Alice, “'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8Mjf19MC-JA",
        "outputId": "9a58b4f6-d8b3-4335-f728-5470257e6c45"
      },
      "source": [
        "matcher = Matcher(nlp.vocab)\n",
        "pattern = [{\"TEXT\": {\"REGEX\": \"^a\"}},{\"POS\": {\"REGEX\": \"^V\"}}] #Reular Expression on filtering Data by text starts with 'a' and follwed by Verb\n",
        "matcher.add(\"country\",[pattern])\n",
        "matches = matcher(doc)\n",
        "print(\"Matches:\", set([doc[start:end].text for match_id, start, end in matches][:10]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Matches: {'and looked', 'about stopping', 'and saying', 'and make', 'and noticed', 'all think', 'all seemed', 'and went', 'and burning', 'are located'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jfWQ1WrsEURf",
        "outputId": "dc4e2ed8-7b1d-4536-c9d2-e583d47ad975"
      },
      "source": [
        "matcher = Matcher(nlp.vocab)\n",
        "pattern = [{\"TEXT\": \"Alice\"}, {\"IS_PUNCT\": True,\"OP\":\"*\"}]\n",
        "matcher.add(\"id1\",[pattern])\n",
        "pattern = [{\"POS\": \"ADJ\"},{\"LOWER\":\"rabbit\"}]\n",
        "matcher.add(\"id2\",[pattern])\n",
        "matches = matcher(doc)\n",
        "print(\"Matches:\", set([doc[start:end].text for match_id, start, end in matches]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Matches: {'Alice;', 'Alice. “', 'Alice,', 'Alice: “—', 'Alice, (', 'Alice.', 'Alice,)', 'Alice', 'Alice!”', 'Alice,) “', 'Alice; “', 'Alice (', 'Alice:', 'Alice: “', 'Alice!', 'Alice, “', 'large rabbit'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8lJWPlJPfG5i",
        "outputId": "e66a3f98-38fb-4931-8817-56c436a40ccd"
      },
      "source": [
        "from spacy import displacy\n",
        "matcher = Matcher(nlp.vocab)\n",
        "matched_sents = []\n",
        "pattern = [{\"LOWER\": \"facebook\"}, {\"LEMMA\": \"be\"}, {\"POS\": \"ADV\", \"OP\": \"*\"}, {\"POS\": \"ADJ\"}]\n",
        "\n",
        "def callback_method_fb(matcher, doc, i, matches):\n",
        "    matched_id, start, end = matches[i]\n",
        "    span = doc[start:end]\n",
        "    sent = span.sent\n",
        "    \n",
        "    match_ents = [{\n",
        "        'start':span.start_char - sent.start_char,                   #character\n",
        "        'end': span.end_char - sent.start_char,\n",
        "        'label': 'MATCH'\n",
        "    }]\n",
        "    \n",
        "    matched_sents.append({'text': sent.text, 'ents':match_ents})\n",
        "    \n",
        "matcher.add(\"fb\",[pattern],on_match=callback_method_fb)\n",
        "doc = nlp(\"I'd say that Facebook is evil. – Facebook is pretty cool, right?\")\n",
        "matches = matcher(doc)\n",
        "matches,matched_sents"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([(8017838677478259815, 4, 7), (8017838677478259815, 9, 13)],\n",
              " [{'ents': [{'end': 29, 'label': 'MATCH', 'start': 13}],\n",
              "   'text': \"I'd say that Facebook is evil.\"},\n",
              "  {'ents': [{'end': 25, 'label': 'MATCH', 'start': 2}],\n",
              "   'text': '– Facebook is pretty cool, right?'}])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YW2OBK93s0Bi",
        "outputId": "4299df10-1d41-4d0a-e14a-05be63bcc7a2"
      },
      "source": [
        "pattern = [{\"ORTH\": \"(\"}, {\"SHAPE\": \"ddd\"}, {\"ORTH\": \")\"}, {\"SHAPE\": \"dddd\"}, {\"ORTH\": \"-\", \"OP\": \"?\"}, {\"SHAPE\": \"dddd\"}]\n",
        "#shape incates digits such as that ddd indicates three digit and dddd indicates four digits\n",
        "#ORTH indicate open bracket\n",
        "matcher = Matcher(nlp.vocab)\n",
        "matcher.add(\"PhoneNumber\",[pattern])\n",
        "\n",
        "doc = nlp(\"Call me at (123) 4560-7890\")\n",
        "\n",
        "#print([t.text for t in doc])\n",
        "matches = matcher(doc)\n",
        "\n",
        "for match_id, start, end in matches:\n",
        "    span = doc[start:end]\n",
        "    print(span.text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(123) 4560-7890\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fh3Fw-65uxDk",
        "outputId": "fb6160ad-dd67-4d1a-9a02-7d2753e3b46c"
      },
      "source": [
        "pattern = [{\"TEXT\": {\"REGEX\": \"[a-zA-Z0-9-_.]+@[a-zA-Z0-9-_.]+\"}}]\n",
        "\n",
        "matcher = Matcher(nlp.vocab)\n",
        "matcher.add(\"Email\", [pattern])\n",
        "\n",
        "text = \"Email me at email2me@kgptalkie.com and talk.me@kgptalkie.com\"\n",
        "doc = nlp(text)\n",
        "\n",
        "matches = matcher(doc)\n",
        "matches\n",
        "for match_id, start, end in matches:\n",
        "    span = doc[start:end]\n",
        "    print(span.text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "email2me@kgptalkie.com\n",
            "talk.me@kgptalkie.com\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r0sS4T2BvGZ4",
        "outputId": "dcb6ee58-aa17-4cc2-97f9-abe81159f073"
      },
      "source": [
        "pos_emoji = [\"😀\", \"😃\", \"😂\", \"🤣\", \"😊\", \"😍\"]  # Positive emoji\n",
        "neg_emoji = [\"😞\", \"😠\", \"😩\", \"😢\", \"😭\", \"😒\"]  # Negative emoji\n",
        "pos_patterns = [[{\"ORTH\": emoji}] for emoji in pos_emoji]\n",
        "neg_patterns = [[{\"ORTH\": emoji}] for emoji in neg_emoji]\n",
        "pos_patterns,neg_patterns"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([[{'ORTH': '😀'}],\n",
              "  [{'ORTH': '😃'}],\n",
              "  [{'ORTH': '😂'}],\n",
              "  [{'ORTH': '🤣'}],\n",
              "  [{'ORTH': '😊'}],\n",
              "  [{'ORTH': '😍'}]],\n",
              " [[{'ORTH': '😞'}],\n",
              "  [{'ORTH': '😠'}],\n",
              "  [{'ORTH': '😩'}],\n",
              "  [{'ORTH': '😢'}],\n",
              "  [{'ORTH': '😭'}],\n",
              "  [{'ORTH': '😒'}]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sBGjerH6vXXS",
        "outputId": "892ac869-45fc-424b-f9ef-29de126869c6"
      },
      "source": [
        "def label_sentiment(matcher, doc, i, matches):\n",
        "    match_id, start, end = matches[i]\n",
        "    if doc.vocab.strings[match_id] == 'HAPPY':\n",
        "        doc.sentiment += 0.1\n",
        "    elif doc.vocab.strings[match_id] == 'SAD':\n",
        "        doc.sentiment -= 0.1\n",
        "matcher = Matcher(nlp.vocab)\n",
        "matcher.add(\"HAPPY\", pos_patterns, on_match=label_sentiment)\n",
        "matcher.add(\"SAD\", neg_patterns, on_match=label_sentiment)\n",
        "matcher.add(\"HASHTAG\", [[{\"ORTH\": \"#\"}, {\"IS_ASCII\": True}]])\n",
        "\n",
        "doc = nlp(\"Hello world 😀 #KGPTalkie\")\n",
        "matches = matcher(doc)\n",
        "\n",
        "for match_id, start, end in matches:\n",
        "    string_id = doc.vocab.strings[match_id]  # Look up string ID\n",
        "    span = doc[start:end]\n",
        "    print(string_id, span.text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "HAPPY 😀\n",
            "HASHTAG #KGPTalkie\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FXDeUTMiz5Uh",
        "outputId": "dad7b4d8-f0dc-45b4-e923-e0da9f2f6e1b"
      },
      "source": [
        "from spacy.matcher import PhraseMatcher\n",
        "\n",
        "matcher = PhraseMatcher(nlp.vocab)\n",
        "terms = ['BARAC OBAMA', 'ANGELA MERKEL', 'WASHINGTON D.C.']\n",
        "pattern = [nlp.make_doc(text) for text in terms]\n",
        "\n",
        "matcher.add(\"TerminologyList\", pattern)\n",
        "doc = nlp(\"German Chancellor ANGELA MERKEL and US President BARAC OBAMA \"\n",
        "          \"converse in the Oval Office inside the White House in WASHINGTON D.C.\")\n",
        "matches = matcher(doc)\n",
        "for match_id, start, end in matches:\n",
        "    span = doc[start:end]\n",
        "    print(span.text)\n",
        "matches"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ANGELA MERKEL\n",
            "BARAC OBAMA\n",
            "WASHINGTON D.C.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(3766102292120407359, 2, 4),\n",
              " (3766102292120407359, 7, 9),\n",
              " (3766102292120407359, 19, 21)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LrOVKgg03_NN",
        "outputId": "f21d79e0-0ddb-4dac-e39b-5feb24754b68"
      },
      "source": [
        "from spacy.lang.en import English\n",
        "\n",
        "nlp = English()\n",
        "ruler = nlp.add_pipe(\"entity_ruler\")\n",
        "patterns = [{\"label\": \"ORG\", \"pattern\": \"Apple\"},\n",
        "            {\"label\": \"GPE\", \"pattern\": [{\"LOWER\": \"san\"}, {\"LOWER\": \"francisco\"}]}]\n",
        "ruler.add_patterns(patterns)\n",
        "\n",
        "doc = nlp(\"Apple is opening its first big office in San Francisco.\")\n",
        "print([(ent.text, ent.label_) for ent in doc.ents])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('Apple', 'ORG'), ('San Francisco', 'GPE')]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}